{"cells":[{"metadata":{},"cell_type":"markdown","source":"### HOUSE SALE PRICING PREDICTION\n\n\nEarlier I had used the linear regression techniques to predict house Prices. The RSME values hovered around 0.15 among various iterations\n\nin this project, I wish to improve the RSME values.\n\n\n**PREPROCESSING & EDA**\n\n- Importing Libraries & Data\n- Dealing with Duplicates and Nan\n- Looking at correlations\n- Data Normalization (Plots & Tests)\n\n\n**MODELING**\n\n- Baseline Models with 10-Folds CV\n- Best Model (RandomGridSearch)\n- Prediction\n- Submission","attachments":{}},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTING LIBRARIES & MAIN PATH\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, norm\nfrom sklearn.neighbors import KNeighborsRegressor\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Defining the working directories\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTING DATA\n\nhouse_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ndata_w = house_data.copy()\ndata_w.columns = data_w.columns.str.replace(' ', '') # Replacing the white spaces in columns' names\ndata_w.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_w.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA & VISUALIZATION\n\nBefore working with any kind of data it is important to understand them. A crucial step to this aim is the ***Exploratory data analysis (EDA)***: a combination of visualizations and statistical analysis (uni, bi, and multivariate) that helps us to better understand the data we are working with and to gain insight into their relationships. So, let's explore our target variable and how the other features influence it."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting the main parameters of the Normal Ditribution ()\n(mu, sigma) = norm.fit(data_w['SalePrice'])\n\nplt.figure(figsize = (12,6))\nsns.distplot(data_w['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skew and kurt\nfrom scipy import stats\n\nshap_t,shap_p = stats.shapiro(data_w['SalePrice'])\n\nprint(\"Skewness: %f\" % abs(data_w['SalePrice']).skew())\nprint(\"Kurtosis: %f\" % abs(data_w['SalePrice']).kurt())\nprint(\"Shapiro_Test: %f\" % shap_t)\nprint(\"Shapiro_Test: %f\" % shap_p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = data_w.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we know which features correlates most with our target variable we can investigate them more in depth."},{"metadata":{"trusted":true},"cell_type":"code","source":"# OverallQuall - SalePrice [Pearson = 0.8]\n\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=data_w, x = 'OverallQual', y='SalePrice', ax = ax[0])\nsns.violinplot(data=data_w, x = 'OverallQual', y='SalePrice', ax = ax[1])\nsns.boxplot(data=data_w, x = 'OverallQual', y='SalePrice', ax = ax[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TotRmsAbvGrd - SalePrice [Pearson = 0.50]\n\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=data_w, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[0])\nsns.violinplot(data=data_w, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[1])\nsns.boxplot(data=data_w, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[2])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GrLivArea vs SalePrice [corr = 0.71]\n\nPearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=data_w, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Pearson_TBSF = 0.63\nplt.figure(figsize = (12,6))\nsns.regplot(data=data_w, x = 'TotalBsmtSF', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('TotalBsmtSF vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_TBSF)], loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# YearBuilt vs SalePrice\n\nPearson_YrBlt = 0.56\nplt.figure(figsize = (12,6))\nsns.regplot(data=data_w, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Median of Sale Price by Year\n\nplt.figure(figsize = (10,5))\nsns.barplot(x='YrSold', y=\"SalePrice\", data = data_w, estimator = np.median)\nplt.title('Median of Sale Price by Year', fontsize = 13)\nplt.xlabel('Selling Year', fontsize = 12)\nplt.ylabel('Median of Price in $', fontsize = 12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  DATA PREPROCESSING\n\nNow that we have some insights about data, we need to preprocess them for the modeling part. The main steps are:\n\n- Looking at potential NaN\n- Dealing with categorical features (e.g. Dummy coding)\n- Normalization\n\nN.B:\n\nUsually, in a real-world project, the test data are not available until the end. For this reason, test data should contain the same type of data of the training set to preprocess them in the same way. Here, the test set is available. It contains some observations not present in the training dataset and,the use of dummy coding could raise several issues (I spent a lot of time figuring out why I was not able to make predictions on the test set). The easiest way to solve this problem (that is not applicable if test data are not available) is to concatenate Train and Test sets, preprocess, and divide them again.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating Target and Features\n\ntarget = data_w['SalePrice']\ntest_id = test['Id']\ntest = test.drop(['Id'],axis = 1)\ndata_w2 = data_w.drop(['SalePrice'], axis = 1)\n\n\n# Concatenating train & test set\n\ntrain_test = pd.concat([data_w2,test], axis=0, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at NaN % within the data\n\nnan = pd.DataFrame(train_test.isna().sum(), columns = ['NaN_sum'])\nnan['feat'] = nan.index\nnan['Perc(%)'] = (nan['NaN_sum']/1460)*100\nnan = nan[nan['NaN_sum'] > 0]\nnan = nan.sort_values(by = ['NaN_sum'])\nnan['Usability'] = np.where(nan['Perc(%)'] > 20, 'Discard', 'Keep')\nnan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting Nan\n\nplt.figure(figsize = (15,5))\nsns.barplot(x = nan['feat'], y = nan['Perc(%)'])\nplt.xticks(rotation=45)\nplt.title('Features containing Nan')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Are we sure that all these nans are real missing values? Looking at the given description file, we can see how the majority of these nans reflect the absence of something, and for this reason, they are not nans. We can impute them (for numerical features) or substitute them with data in the file:"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\n\n# Filling Categorical NaN (That we know how to fill due to the description file )\n\ntrain_test['Functional'] = train_test['Functional'].fillna('Typ')\ntrain_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\ntrain_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\ntrain_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\ntrain_test[\"PoolQC\"] = train_test[\"PoolQC\"].fillna(\"None\")\ntrain_test[\"Alley\"] = train_test[\"Alley\"].fillna(\"None\")\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\ntrain_test['Fence'] = train_test['Fence'].fillna(\"None\")\ntrain_test['MiscFeature'] = train_test['MiscFeature'].fillna(\"None\")\n\nfor col in ('GarageArea', 'GarageCars'):\n    train_test[col] = train_test[col].fillna(0)\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train_test[col] = train_test[col].fillna('None')\n    \n    # Checking the features with NaN remained out\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the useless variables\n\nuseless = ['GarageYrBlt','YearRemodAdd'] \ntrain_test = train_test.drop(useless, axis = 1)\n\n# Imputing with KnnRegressor (we can also use different Imputers)\n\ndef impute_knn(df):\n    ttn = train_test.select_dtypes(include=[np.number])\n    ttc = train_test.select_dtypes(exclude=[np.number])\n\n    cols_nan = ttn.columns[ttn.isna().any()].tolist()         # columns w/ nan \n    cols_no_nan = ttn.columns.difference(cols_nan).values     # columns w/n nan\n\n    for col in cols_nan:\n        imp_test = ttn[ttn[col].isna()]   # indicies which have missing data will become our test set\n        imp_train = ttn.dropna()          # all indicies which which have no missing data \n        model = KNeighborsRegressor(n_neighbors=5)  # KNR Unsupervised Approach\n        knr = model.fit(imp_train[cols_no_nan], imp_train[col])\n        ttn.loc[ttn[col].isna(), col] = knr.predict(imp_test[cols_no_nan])\n    \n    return pd.concat([ttn,ttc],axis=1)\n\ntrain_test = impute_knn(train_test)\n\n\nobjects = []\nfor i in train_test.columns:\n    if train_test[i].dtype == object:\n        objects.append(i)\ntrain_test.update(train_test[objects].fillna('None'))\n\n# # Checking NaN presence\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FEATURE ENGINEERING\n\nLet's create some new features combining the ones that we already have. These could help us to increase the performance of the model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test[\"SqFtPerRoom\"] = train_test[\"GrLivArea\"] / (train_test[\"TotRmsAbvGrd\"] +\n                                                       train_test[\"FullBath\"] +\n                                                       train_test[\"HalfBath\"] +\n                                                       train_test[\"KitchenAbvGr\"])\n\ntrain_test['Total_Home_Quality'] = train_test['OverallQual'] + train_test['OverallCond']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test[\"HighQualSF\"] = train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"]\n\n# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\n\n# Creating dummy variables from categorical features\n\ntrain_test_dummy = pd.get_dummies(train_test)\n\n# Fetch all numeric features\n\n#train_test['Id'] = train_test['Id'].apply(str)\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's try to tranform our target distribution into a normal one. To do this we use a log transformation. We will use qq-plot to see the transformation effect.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SalePrice after transformation\n\ntarget_log = np.log1p(target)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MODELING"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nimport xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-Test separation\n\ntrain = train_test_dummy[0:1460]\ntest = train_test_dummy[1460:]\ntest['Id'] = test_id\n\n# Creation of the RMSE metric:\n    \ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_dummy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\n\nbaseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n                   'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                   'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.']\n\n# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())\n\n# Bayesian Ridge Regression\n\nbrr = BayesianRidge(compute_score=True)\nscore_brr = cv_rmse(brr)\ncv_scores.append(score_brr.mean())\ncv_std.append(score_brr.std())\n\n# Light Gradient Boost Regressor\n\nl_gbm = LGBMRegressor(objective='regression')\nscore_l_gbm = cv_rmse(l_gbm)\ncv_scores.append(score_l_gbm.mean())\ncv_std.append(score_l_gbm.std())\n\n# Support Vector Regression\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\ncv_scores.append(score_svr.mean())\ncv_std.append(score_svr.std())\n\n# Decision Tree Regressor\n\ndtr = DecisionTreeRegressor()\nscore_dtr = cv_rmse(dtr)\ncv_scores.append(score_dtr.mean())\ncv_std.append(score_dtr.std())\n\n# Random Forest Regressor\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\ncv_scores.append(score_rfr.mean())\ncv_std.append(score_rfr.std())\n\n# XGB Regressor\n\nxgb = xgb.XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())\n\n# Cat Boost Regressor\n\ncatb = CatBoostRegressor()\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())\n\n# Stacked Regressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          LinearRegression(),\n                                          BayesianRidge(),\n                                          GradientBoostingRegressor()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\n\nscore_stack_gen = cv_rmse(stack_gen)\ncv_scores.append(score_stack_gen.mean())\ncv_std.append(score_stack_gen.std())\n\nfinal_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-Test split the data\n\nX_train,X_val,y_train,y_val = train_test_split(train,target_log,test_size = 0.1,random_state=42)\n\n# Cat Boost Regressor\n\ncat = CatBoostRegressor()\ncat_model = cat.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_pred = cat_model.predict(X_val)\ncat_score = rmse(y_val, cat_pred)\ncat_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's take a look at the top 20 most important variables for our model. This could give us further insight into the functioning of the algorithm and how and which data it uses most to arrive at the final prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features' importance of our model\n\nfeat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting top 20 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature importance Interactive Plot \n\ntrain_pool = Pool(X_train)\nval_pool = Pool(X_val)\n\nexplainer = shap.TreeExplainer(cat_model) # insert your model\nshap_values = explainer.shap_values(train_pool) # insert your train Pool object\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[:200,:], X_train.iloc[:200,:])\n\n# The plot represents just a slice of the Training data (200 observations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above diagram represents each observation (x-axis) for the feature presented (y-axis). The x location of each dot on the x-axis reflects the impact of that feature on the model's predictions, while the color of the dot represents the value of that feature for that exact observation. Dots that pile up on the line show density. Here we can see how features such as 'BsmtFinType1_GLQ' or 'BsmtQual_Ex', differently from 'GrLivArea' and 'OverallQual', do not contribute significantly in producing the final predictions.\n\n"},{"metadata":{},"cell_type":"markdown","source":" N.B: Catboost comes with a great method: ***get_feature_importance***. This method can be used to find important interactions among features. This is a huge advantage because it can give us insights about possible new features to create that can improve the performance.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features' Interactions\n\ntrain_data = Pool(X_train)\n\ninteraction = cat_model.get_feature_importance(train_data, type=\"Interaction\")\ncolumn_names = X_train.columns.values \ninteraction = pd.DataFrame(interaction, columns=[\"feature1\", \"feature2\", \"importance\"])\ninteraction.feature1 = interaction.feature1.apply(lambda l: column_names[int(l)])\ninteraction.feature2 = interaction.feature2.apply(lambda l: column_names[int(l)])\ninteraction.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which are the deafult parameters used by CaboostRegressor? This is our real baseline, now we need to optimize the hyperparameters trying to tune the model to obtain a better performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Catboost default paramters\n\ncat_model.get_all_params()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Optimization"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preforming a Random Grid Search to find the best combination of parameters\n\ngrid = {'iterations': [1000,6000],\n        'learning_rate': [0.05, 0.005, 0.0005],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 3, 5, 9]}\n\nfinal_model = CatBoostRegressor()\nrandomized_search_result = final_model.randomized_search(grid,\n                                                   X = X_train,\n                                                   y= y_train,\n                                                   verbose = False,\n                                                   plot=True)\n                                                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final Cat-Boost Regressor\n\nparams = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = False)\n\ncatf_pred = cat_model_f.predict(X_val)\ncatf_score = rmse(y_val, catf_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"catf_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SUBMISSION"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Test CSV Submission\n\ntest_pred = cat_f.predict(test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the results in a csv file\n\nsubmission.to_csv(\"result.csv\", index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}